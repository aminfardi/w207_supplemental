{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_features_pipeline(data, target_file, temp_increase_vars):\n",
    "    '''\n",
    "    Function to run all of our preprocessing functionalities, with a few different configuration options.\n",
    "    Will save a new dataframe as csv file in a specified address\n",
    "    \n",
    "    INPUTS:\n",
    "    data: a pandas dataframe containing the raw data\n",
    "    target_file: a file name and address to save the preprocessed data frames. expects .csv extension\n",
    "    temp_increase_vars: a boolean value (True or False) on whether or not to create the binary temperature increase variables\n",
    "    \n",
    "    OUTPUTS:\n",
    "    data: a data frame with the preprocessed data\n",
    "    file saved... the preprocessed data frame saved as a csv file at the specified file extension\n",
    "    '''\n",
    "    \n",
    "    #Add campaign duration and campaignID columns\n",
    "    #Need to convert column to date_time type\n",
    "    data['Date.Time'] = pd.to_datetime(data['Date.Time'])\n",
    "\n",
    "    #Loop through and identify consecutive dates. Create a list of campaign IDs to then create a column from.\n",
    "    campaign_num = 1\n",
    "    campaign_id_list = [1]\n",
    "    for index in range(1,len(data['Date.Time'])):\n",
    "        if (data['Date.Time'][index] - data['Date.Time'][index-1]).days < 1:\n",
    "            campaign_id_list.append(campaign_num)\n",
    "        else:\n",
    "            campaign_num += 1\n",
    "            campaign_id_list.append(campaign_num)\n",
    "\n",
    "    #Create new column of campaign IDs:\n",
    "    data['CampaignID'] = campaign_id_list\n",
    "\n",
    "    #Loop through and calculate hours in campaign\n",
    "    duration_list = [0]\n",
    "\n",
    "    for index in range(1,len(data['Date.Time'])):\n",
    "        campaign_num = data['CampaignID'][index]\n",
    "        campaign_start = min(data['Date.Time'][data['CampaignID'] == campaign_num])\n",
    "        hrs_since_start = (data['Date.Time'][index] - campaign_start).delta*2.77778e-13\n",
    "        duration_list.append(hrs_since_start)\n",
    "\n",
    "    #Create new column for campaign hours\n",
    "    data['Campaign.Hrs'] = duration_list\n",
    "    \n",
    "    dates = data['Date.Time']\n",
    "    data.drop(['Unnamed: 0', 'Date.Time'], axis = 1, inplace = True)\n",
    "    \n",
    "    data.drop(['Pressure'], axis = 1, inplace = True)\n",
    "    \n",
    "    #delete rows where Main_Mass_Flow =0\n",
    "    rows_to_delete = np.where(data.Main_Mass_Flow == 0)[0]\n",
    "    # also add 337, 2533\n",
    "    rows_to_delete = np.sort(np.concatenate((rows_to_delete, np.array([337, 2533])), axis = 0))\n",
    "\n",
    "    data.drop(rows_to_delete, axis = 0, inplace = True)\n",
    "    \n",
    "    # replace T_Zone_1 zero vals with median\n",
    "    new_zero_val_T = np.median(data.T_Zone_1.iloc[np.where(data.T_Zone_1 > 50)])\n",
    "    data.T_Zone_1.iloc[np.where(data.T_Zone_1 <= 50)] = new_zero_val_T\n",
    "\n",
    "    # replace Blending vals with median\n",
    "    new_zero_val_B = np.median(data.Blending.iloc[np.where(data.Blending > 20)])\n",
    "    data.Blending.iloc[np.where(data.Blending <= 20)] = new_zero_val_B\n",
    "    \n",
    "    data = data[data.Main_Mass_Flow > 15000]\n",
    "    \n",
    "    if temp_increase_vars:\n",
    "        for i in range(1,10):\n",
    "            new_colname = \"T_Increase_\" + str(i)\n",
    "            colname_i = \"T_Zone_\" + str(i)\n",
    "            colname_inext = \"T_Zone_\" + str(i+1)\n",
    "            vals = (data[colname_i] <= data[colname_inext]).astype(int)\n",
    "            data[new_colname] = vals\n",
    "   \n",
    "    data.to_csv(target_file)\n",
    "    return data\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_training(data, scaler):\n",
    "    '''\n",
    "    A function that will peform scaling on training data given that all features are already created.\n",
    "    \n",
    "    INPUTS:\n",
    "    data: a pandas dataframe to be scaled\n",
    "    scaler: a scaling protocol: \"standard\" or \"minmax\"\n",
    "    \n",
    "    RETURNS:\n",
    "    scaler: a scaler object that can be used to scale testing data\n",
    "    scaled_features: a scaled pandas dataframe object\n",
    "    '''\n",
    "    if scaler == \"standard\":\n",
    "        scaled_colnames = data.columns\n",
    "        scaled_features = data.copy()\n",
    "        features = scaled_features[scaled_colnames]\n",
    "        scaler = StandardScaler().fit(features.values)\n",
    "        features = scaler.transform(features.values)\n",
    "        scaled_features[scaled_colnames] = features\n",
    "        \n",
    "    elif scaler == \"minmax\":\n",
    "        scaled_colnames = data.columns\n",
    "        scaled_features = data.copy()\n",
    "        features = scaled_features[scaled_colnames]\n",
    "        scaler = MinMaxScaler().fit(features.values)\n",
    "        features = scaler.transform(features.values)\n",
    "        scaled_features[scaled_colnames] = features\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(scaler)\n",
    "        \n",
    "    return scaler, scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"anonymized_SAP_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_features_pipeline(data, 'no_tempinc.csv', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_features_pipeline(data, 'tempinc.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = ['no_tempinc.csv', 'tempinc.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = ['standard','minmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in data_files:\n",
    "    data = pd.read_csv(str('./p_data/')+file)\n",
    "    for scaler in scalers:\n",
    "        sc, temp_df = scale_training(data,scaler)\n",
    "        file_name = file[:-4]+ str('_') + scaler + str('.csv')\n",
    "        temp_df.to_csv('./p_data/'+file_name)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
